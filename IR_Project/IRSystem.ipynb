{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39d2b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLower(text):\n",
    "    ''' Convert text to lower case'''\n",
    "    return text.lower()\n",
    "#toLower(df.loc[1,'title'])\n",
    "################################################################\n",
    "from decimal import Decimal\n",
    "import re\n",
    "import inflect\n",
    "\n",
    "\n",
    "p = inflect.engine()\n",
    "reg = r'([0-9]+)'\n",
    "\n",
    "def isFLoat(strNum):\n",
    "    '''converte float number to word'''\n",
    "    try:\n",
    "        float(strNum)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "##########################################################\n",
    "\n",
    "def convertNumbers(text):\n",
    "    ''' Convert texnumbers to words'''\n",
    "    tempText = text.split()\n",
    "    newText = []\n",
    "    for word in tempText:\n",
    "        tempList = re.split(reg,word)\n",
    "        for miniWord in tempList:\n",
    "            if miniWord.isdigit() or isFLoat(miniWord):\n",
    "                temp = p.number_to_words(miniWord)\n",
    "                newText.append(removePunctuation(temp))\n",
    "            else:\n",
    "                newText.append(miniWord)        \n",
    "    tempText = ' '.join(newText)\n",
    "    return tempText\n",
    "##################################################################\n",
    "def removeWhiteSpace(text):\n",
    "    '''remove whitespace from text'''\n",
    "    return \" \".join(text.split())\n",
    "#removeWhiteSpace(\"hi   hi\")\n",
    "######################################################################\n",
    "import string\n",
    "translator = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "def removePunctuation(text):\n",
    "    ''' remove punctuation from text'''\n",
    "    global translator\n",
    "    return text.translate(translator)\n",
    "#removePunctuation(df.loc[1,'title'])\n",
    "##############################################################\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def removeStopWords(text):\n",
    "    ''' remove stopwords from text'''\n",
    "    sw = set(stopwords.words(\"english\"))\n",
    "    wt = word_tokenize(text)\n",
    "    filteredText = [word for word in wt if word not in sw]\n",
    "    return ' '.join(filteredText)\n",
    "#print(df.loc[1,'title'])\n",
    "#print(removeStopWords(df.loc[0,'title']))\n",
    "###############################################################\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stemmingWords(text):\n",
    "    ''' stemm words'''\n",
    "    global stemmer\n",
    "    wt = word_tokenize(text)\n",
    "    stems = []\n",
    "    for word in wt:\n",
    "        temp = stemmer.stem(word)\n",
    "        stems.append(temp)\n",
    "    return ' '.join(stems)\n",
    "#print(stemmingWords(df.loc[0,'title']))\n",
    "###############################################################\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, defaultdict\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "def lemmatizeWords(text):\n",
    "    ''' lemmatize words'''\n",
    "    tokens = word_tokenize(text)\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmas = [lmtzr.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens) ]\n",
    "    return ' '.join(lemmas)\n",
    "#print(lemmatizeWords(df.loc[0,'title']))\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61903c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def titlePreProcesse(text):\n",
    "    \n",
    "    tempText = toLower(text)\n",
    "    tempText = removePunctuation(tempText)\n",
    "    tempText = convertNumbers(tempText)\n",
    "    tempText = removeWhiteSpace(tempText)\n",
    "    tempText = removeStopWords(tempText)\n",
    "    tempText = lemmatizeWords(tempText)\n",
    "    return tempText\n",
    "#print(titlePreProcesse(df.loc[0,'title']))\n",
    "###########################################################\n",
    "\n",
    "def preprocessedAntiqueData(dataFrame:pd.DataFrame):\n",
    "    tempFrame = pd.DataFrame(columns = ['id', 'title'])\n",
    "    tempFrame1 = pd.DataFrame(columns = ['id', 'title'])\n",
    "    tempFrame2 = pd.DataFrame(columns = ['id', 'title'])\n",
    "    tempFrame3 = pd.DataFrame(columns = ['id', 'title'])\n",
    "    tempFrame4 = pd.DataFrame(columns = ['id', 'title'])\n",
    "\n",
    "    for i in dataFrame.index[0:100000:1]:\n",
    "        try:\n",
    "            tempT = titlePreProcesse(dataFrame.loc[i, 'title'])\n",
    "            dataf=pd.DataFrame([[dataFrame.loc[i,'id'],tempT]],columns=['id','title'])\n",
    "            tempFrame1 = pd.concat([tempFrame1,dataf], ignore_index=True)\n",
    "        except:\n",
    "            print(i,dataFrame.loc[i, 'title'])\n",
    "            continue\n",
    "            #raise \n",
    "\n",
    "    for i in dataFrame.index[100000:200000:1]:\n",
    "        try:\n",
    "            tempT = titlePreProcesse(dataFrame.loc[i, 'title'])\n",
    "            dataf=pd.DataFrame([[dataFrame.loc[i,'id'],tempT]],columns=['id','title'])\n",
    "            tempFrame2 = pd.concat([tempFrame2,dataf], ignore_index=True)\n",
    "        except:\n",
    "            print(i,dataFrame.loc[i, 'title'])\n",
    "            continue\n",
    "            #raise  \n",
    "      \n",
    "    for i in dataFrame.index[200000:300000:1]:\n",
    "        try:\n",
    "            tempT = titlePreProcesse(dataFrame.loc[i, 'title'])\n",
    "            dataf=pd.DataFrame([[dataFrame.loc[i,'id'],tempT]],columns=['id','title'])\n",
    "            tempFrame3 = pd.concat([tempFrame3,dataf], ignore_index=True)\n",
    "        except:\n",
    "            print(i,dataFrame.loc[i, 'title'])\n",
    "            continue\n",
    "            #raise  \n",
    "      \n",
    "    for i in dataFrame.index[300000::1]:\n",
    "        try:\n",
    "            tempT = titlePreProcesse(dataFrame.loc[i, 'title'])\n",
    "            dataf=pd.DataFrame([[dataFrame.loc[i,'id'],tempT]],columns=['id','title'])\n",
    "            tempFrame4 = pd.concat([tempFrame4,dataf], ignore_index=True)\n",
    "        except:\n",
    "            print(i,dataFrame.loc[i, 'title'])\n",
    "            continue\n",
    "            #raise   \n",
    "    \n",
    "    tempFrame=pd.concat([tempFrame1,tempFrame2,tempFrame3,tempFrame4],ignore_index=True)\n",
    "    tempFrame.fillna('', inplace=True)\n",
    "    return tempFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a60051ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def QueryTitlePreProcesse(a):\n",
    "    tempText = toLower(a)\n",
    "    tempText = removePunctuation(tempText)\n",
    "    tempText = removeWhiteSpace(tempText)\n",
    "    tempText = removeStopWords(tempText)\n",
    "    tempText = stemmingWords(tempText)\n",
    "    tempText = lemmatizeWords(tempText)\n",
    "    return tempText\n",
    "########################################################\n",
    "def preprocesseQuery(dataFrame:pd.DataFrame):\n",
    "    tempFrame = pd.DataFrame(columns = ['id', 'title'])\n",
    "    for i in dataFrame.index:\n",
    "        try:\n",
    "            tempTilte=queryTitlePreProcesse(dataFrame.loc[i,'title'])\n",
    "            tempFrame.loc[len(tempFrame.index)] = [dataFrame.loc[i,'id'],tempTitle]\n",
    "        except:\n",
    "            print(i)\n",
    "            continue\n",
    "            #raise \n",
    "    \n",
    "    tempFrame.fillna('', inplace=True)\n",
    "    return tempFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0293bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "########################################################################\n",
    "\n",
    "transformer = None\n",
    "tfidfTable  = None\n",
    "def initializeTfidfTable(data: pd.DataFrame):\n",
    "    global transformer, tfidfTable\n",
    "    transformer = FeatureUnion([('title_tfidf', \n",
    "                      Pipeline([\n",
    "                        ('extract_field',\n",
    "                                  FunctionTransformer(lambda x: x['title'], \n",
    "                                                      validate=False)),\n",
    "                                ('tfidf', \n",
    "                                  TfidfVectorizer(norm='l1' ,ngram_range=(1,2)))]))                              \n",
    "                      \n",
    "    ])\n",
    "    tfidfTable = transformer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a50d6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words:list=[]\n",
    "def readWords(data:pd.DataFrame):\n",
    "    global words\n",
    "    for id in data.index[1:]:\n",
    "        tempWords=[]\n",
    "        for word in data.loc[id,'title'].split():\n",
    "            tempWords.append(word)\n",
    "        words.append(tempWords)\n",
    "#readWords(processedData)\n",
    "\n",
    "#########################################################\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=None\n",
    "def createModel(words:list):\n",
    "    global model\n",
    "    model=gensim.models.Word2Vec(words,min_count=1,vector_size=300,window=5)\n",
    "    model.train(words, total_examples=len(words), epochs=10)\n",
    "    model.save('WEmodel_final_training_words.bin')\n",
    "    print(\"model is created\")\n",
    "#createModel(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7e22818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e106943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vectors:list=[]\n",
    "    \n",
    "def makeVectors(dataFrame:pd.DataFrame()):\n",
    "    global vectors\n",
    "    try:\n",
    "        for id in dataFrame.index:\n",
    "            avgword2vec =None\n",
    "            count=0\n",
    "            tempTitle=dataFrame.loc[id,'title']\n",
    "            for word in tempTitle.split():\n",
    "                if word in model.wv:\n",
    "                    count+=1\n",
    "                    if avgword2vec is None:\n",
    "                        avgword2vec  =model.wv[word]\n",
    "                    else:\n",
    "                        avgword2vec =avgword2vec + model.wv[word]\n",
    "\n",
    "            if avgword2vec is not None:\n",
    "                avgword2vec = avgword2vec / count\n",
    "\n",
    "                vectors.append(avgword2vec)\n",
    "            else:\n",
    "                vectors.append([0]*300) \n",
    "        #print(len(te), len(vectors))\n",
    "        \n",
    "    except:\n",
    "        print(\"error\")\n",
    "#makeVectors(processedData)\n",
    "##################################################################\n",
    "def makeVectorQuery(query:str):\n",
    "    try:\n",
    "        avg=None\n",
    "        count=0\n",
    "        for word in query.split():\n",
    "            if word in model.wv:\n",
    "                count+=1\n",
    "                if avg is None:\n",
    "                    avg=model.wv[word]\n",
    "                else:\n",
    "                    avg=avg+model.wv[word]\n",
    "        if avg is not None:\n",
    "            avg=avg/count\n",
    "            return [avg]\n",
    "    except:\n",
    "        print('error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10fdcf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "\n",
    "def matching(query):\n",
    "    global transformer, tfidfTable\n",
    "    querytfidf = transformer.transform(query)\n",
    "    cosine_sim=cosine_similarity(querytfidf,tfidfTable).flatten()\n",
    "    return cosine_sim\n",
    "    \n",
    "#################################################################\n",
    "import numpy as np\n",
    "def matchingWE(query)->np.ndarray:\n",
    "    global vectors\n",
    "    vecQuery=makeVectorQuery(query)\n",
    "    cosine_sim=cosine_similarity(vecQuery,vectors).flatten()\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d7bf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesseSearchInput(query) -> pd.DataFrame:\n",
    "    \n",
    "    #data = dataDic.get('query')\n",
    "    tempI = 1\n",
    "    tempW = QueryTitlePreProcesse(query)\n",
    "    tempFrame = pd.DataFrame(columns = ['id', 'title'])\n",
    "    tempFrame.loc[len(tempFrame.index)] = [tempI,tempW]\n",
    "    tempFrame.fillna('', inplace=True)\n",
    "    return tempFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11b4f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absSub(a,b):\n",
    "    return abs(a-b)\n",
    "######################################################\n",
    "def search(qDF:pd.DataFrame,data:pd.DataFrame, n):\n",
    "    ''' search for input and return list of ids of the result'''\n",
    "    try:\n",
    "        resultlis = []\n",
    "        similars = matching(qDF)\n",
    "        tempIds = similars.argsort(axis=0)[-n:][::-1]\n",
    "        tempFrame = pd.DataFrame(columns = ['id', 'title'])\n",
    "        for i in tempIds:\n",
    "            tempFrame.loc[len(tempFrame.index)] = [data.loc[i,'id'],data.loc[i,\"title\"]]\n",
    "        return tempFrame\n",
    "    except:\n",
    "        print(\"false\")\n",
    "        raise\n",
    "##########################################################\n",
    "def searchWE(qDF:pd.DataFrame,data:pd.DataFrame, n):\n",
    "    ''' search for input and return list of ids of the result'''\n",
    "    try:\n",
    "        resultlis = []\n",
    "        similars = matchingWE(qDF.loc[0,'title'])\n",
    "        tempIds = similars.argsort(axis=0)[-n:][::-1]\n",
    "        tempFrame = pd.DataFrame(columns = ['id', 'title'])\n",
    "        for i in tempIds:\n",
    "            tempFrame.loc[len(tempFrame.index)] = [data.loc[i,'id'],data.loc[i,\"title\"]]\n",
    "        return tempFrame\n",
    "    except:\n",
    "        print(\"false\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5db8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInput(typeSearch,query):\n",
    "    dataPD: pd.DataFrame = preprocesseSearchInput(query)\n",
    "    if  typeSearch==2:\n",
    "        resultIds = searchWE(dataPD, dataFrame, 10)\n",
    "    else:\n",
    "        resultIds = search(dataPD, dataFrame, 10) \n",
    "    return resultIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "589a31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "def reSizeLists(l1:list, l2:list, n :int):\n",
    "    '''resize lists to have the same len'''\n",
    "    \n",
    "    if len(l1) < len(l2):\n",
    "        l2 = l2[0:len(l1)]\n",
    "    while len(l1) > len(l2):\n",
    "        l1 = l1[0:len(l2)]\n",
    "    if len(l1)<n:\n",
    "        n=len(l1)\n",
    "    return l1[0:n], l2[0:n]\n",
    "\n",
    "def getQrelsArray( qrelsData: pd.DataFrame,qid):\n",
    "    qrelsArray:list = []\n",
    "    for index in qrelsData.index:\n",
    "        if qrelsData.loc[index,'query_id']==qid:\n",
    "            qrelsArray.append(qrelsData.loc[index,'relevance'])\n",
    "    return qrelsArray\n",
    "\n",
    "def getRes( resData: pd.DataFrame,qid):\n",
    "    resArray:list = []\n",
    "    for index in resData.index[1::]:\n",
    "        if resData.loc[index,'qid']==qid:\n",
    "            resArray.append(resData.loc[index,'rid'])\n",
    "    return resArray\n",
    "\n",
    "def precisionAtK(resData:pd.DataFrame, qrelsData: pd.DataFrame,n):\n",
    "    precisionsAtK:list = []\n",
    "    for i in resData.index[1::10]:\n",
    "        resArray:list = []\n",
    "        x=i+10 \n",
    "        for inde in range(int(i),int(x)):\n",
    "            resArray.append(resData.loc[inde,'rid'])\n",
    "        #resArray=getRes(resData,resData.loc[i,'qid'])\n",
    "        qrelsArray=getQrelsArray(qrelsData,resData.loc[i,'qid'])\n",
    "        if len(qrelsArray) == 0: \n",
    "            continue\n",
    "        #reSet=set(resArray)\n",
    "        #qrSet=set(qrelsArray)\n",
    "        #precisionsAtK.append(len(reSet&qrSet)/len(reSet))\n",
    "        resArray, qrelsArray = reSizeLists(resArray, qrelsArray,n)\n",
    "        precisionsAtK.append(precision_score(qrelsArray, resArray, average='micro'))\n",
    "    return sum(precisionsAtK) / len(precisionsAtK)\n",
    "\n",
    "\n",
    "def calPrecisionAtK(querData:pd.DataFrame,resData:pd.DataFrame, qrelsData: pd.DataFrame,n):\n",
    "    precisionsAtK:list = []\n",
    "    for i in querData.index[1::]:\n",
    "        resArray:list = []\n",
    "        resArray=getRes(resData,querData.loc[i,'query_id'])\n",
    "        \n",
    "        if len(resArray) == 0 : \n",
    "            print(\"res=0\")\n",
    "            continue\n",
    "        qrelsArray=getQrelsArray(qrelsData,querData.loc[i,'query_id'])\n",
    "        if len(qrelsArray) == 0 : \n",
    "            continue\n",
    "        \n",
    "        #resArray, qrelsArray = reSizeLists(resArray, qrelsArray,n)\n",
    "        reSet=set(resArray)\n",
    "        qrSet=set(qrelsArray)\n",
    "        precisionsAtK.append((reSet&qrSet)/len(reSet))\n",
    "        #precisionsAtK.append(precision_score(qrelsArray, resArray, average='micro'))\n",
    "    return sum(precisionsAtK) / len(precisionsAtK)\n",
    "\n",
    "\n",
    "def recallsAtK(resData:pd.DataFrame, qrelsData: pd.DataFrame,n):\n",
    "    recallsAtK:list = []\n",
    "    for i in resData.index[1::10]:\n",
    "        resArray:list = []\n",
    "        for inde in range(int(i),int(i+10)):\n",
    "            resArray.append(resData.loc[inde,'rid'])\n",
    "        qrelsArray=getQrelsArray(qrelsData,resData.loc[i,'qid'])\n",
    "        if len(qrelsArray) == 0: \n",
    "            continue\n",
    "        resArray, qrelsArray = reSizeLists(resArray, qrelsArray,n)\n",
    "        recallsAtK.append(recall_score(qrelsArray, resArray, average='micro'))\n",
    "    return sum(recallsAtK) / len(recallsAtK)\n",
    "\n",
    "\n",
    "def MAPAtK(resData:pd.DataFrame, qrelsData: pd.DataFrame):\n",
    "    APAtk:list=[]\n",
    "    for i in resData.index[1::10]:\n",
    "        resArray:list = []   \n",
    "        for inde in range(int(i),int(i)+10):\n",
    "            resArray.append(resData.loc[inde,'rid'])\n",
    "        qrelsArray=getQrelsArray(qrelsData,resData.loc[i,'qid'])\n",
    "        if len(qrelsArray) == 0: \n",
    "            continue\n",
    "        resArray, qrelsArray = reSizeLists(resArray, qrelsArray,n)\n",
    "        for id in range(len(resArray)):\n",
    "            tempresArray, tempqrelsArray = reSizeLists(resArray, qrelsArray,id)\n",
    "            APAtk.append(precision_score(tempqrelsArray,tempresArray,average='micro'))\n",
    "    return sum(APAtk) / len(APAtk)\n",
    "\n",
    "def MRR(resData:pd.DataFrame,qrelsData: pd.DataFrame):\n",
    "    mrr:list=[]\n",
    "    for i in resData.index[1::10]:\n",
    "        resArray:list = []\n",
    "        for inde in range(int(i),int(i)+10):\n",
    "            resArray.append(resData.loc[inde,'rid'])\n",
    "        qrelsArray=getQrelsArray(qrelsData,resData.loc[i,'qid'])\n",
    "        \n",
    "        for id in range(len(resArray)):\n",
    "            if resArray[id] in qrelsArray:\n",
    "                mrr.appent(1/(id+1))\n",
    "                break\n",
    "    \n",
    "    return sum(mrr)/len(mrr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b3f88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executionQuery(qDataFrame:pd.DataFrame,data:pd.DataFrame, n,typeSearch):\n",
    "    ''' search for all queries in the queries file and get the most n similar document .I'''\n",
    "    result = pd.DataFrame(columns=['qid','rid','rank'])\n",
    "    \n",
    "    for i in qDataFrame.index[1:250]:\n",
    "        tempFrame =pd.DataFrame(columns=['qid','rid','rank'])\n",
    "        try:\n",
    "            resultDict:dict = {}\n",
    "            #tempIds:list = matching(pd.DataFrame(qDataFrame.loc[qDataFrame.index == i,:]))\n",
    "            tempW = QueryTitlePreProcesse(qDataFrame.loc[i,'title'])\n",
    "            tempFrame1= pd.DataFrame(columns = ['id', 'title'])\n",
    "            tempFrame1.loc[len(tempFrame.index)] = [1,tempW]\n",
    "            \n",
    "            #query=QueryTitlePreProcesse(qDataFrame.loc[i,'title'])\n",
    "            if typeSearch==1:\n",
    "                tempIds:list = matchingWE(tempFrame1)\n",
    "            else:\n",
    "                tempIds:list = matchingWE(tempW)\n",
    "            tempIds = tempIds.argsort(axis=0)[-n:][::-1] \n",
    "            tempList = []\n",
    "            qid=qDataFrame.loc[i,'query_id']\n",
    "            for id in tempIds[0:n:]:\n",
    "                #dataf=pd.DataFrame([qid,id])\n",
    "                tempFrame.loc[len(tempFrame.index)] = [qid,data.loc[id,'id'],id]\n",
    "            result = pd.concat([result,tempFrame], ignore_index=True)\n",
    "                \n",
    "        except:\n",
    "            print(i)\n",
    "            #raise\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da7a68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def readdata():\n",
    "    qDataFrame:pd.DataFrame = pd.read_table('antique/collection.tsv')\n",
    "    qDataFrame.fillna('', inplace=True)\n",
    "    return qDataFrame\n",
    "###################################################################\n",
    "\n",
    "def readPPData():\n",
    "    qDataFrame:pd.DataFrame = pd.read_table('antique/preprocessData.tsv')\n",
    "    qDataFrame.fillna('', inplace=True)\n",
    "    return qDataFrame\n",
    "###################################################################\n",
    "\n",
    "def readQueryFile():\n",
    "    qDataFrame:pd.DataFrame = pd.read_table('antique/train/queries.txt',names=[\"query_id\",\"title\"])\n",
    "    qDataFrame.fillna('', inplace=True)\n",
    "    return qDataFrame\n",
    "###################################################################\n",
    "\n",
    "def readQrelsFile():\n",
    "    qDataFrame:pd.DataFrame = pd.read_table('antique/train/qrels.tsv',names=['query_id','doc_id', 'relevance','iteration'],delim_whitespace=True)\n",
    "    qDataFrame.fillna('', inplace=True)\n",
    "    return qDataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54bbbee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFiles():\n",
    "    global dataFrame\n",
    "    dataFrame=readdata()\n",
    "    PPdata=readPPData()\n",
    "    queriesDataFrame=readQueryFile()\n",
    "    qrelsDataFrame=readQrelsFile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7435dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initvectors():\n",
    "    global model\n",
    "    #readWords(PPdata)\n",
    "    #createModel(words)\n",
    "    model = Word2Vec.load('WEmodel_final_training_words.bin')\n",
    "    makeVectors(PPdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "146d2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataFrame:pd.DataFrame\n",
    "def init():\n",
    "    global dataFrame\n",
    "    readFiles()\n",
    "    initializeTfidfTable(dataFrame)\n",
    "    initvectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84f04170",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = {\n",
    "    \"query\":\"world\",\n",
    "    \"type\":2\n",
    "}\n",
    "#searchInput(2,\"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ee16795",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execution():\n",
    "    re=executionQuery(queriesDataFrame,PPdata,10,2)\n",
    "    re.to_csv('antique/train/resultWE.tsv',sep=\"\\t\")\n",
    "    print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42347148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evav():\n",
    "    n=10\n",
    "\n",
    "    re:pd.DataFrame = pd.read_table('antique/train/resultWE.tsv',names=['qid','rid','rank'])\n",
    "\n",
    "    precisionAtK = precisionAtK(re, qrelsDataFrame,n)\n",
    "    print(f'precacmon@{n} : {precisionAtK}')\n",
    "    recallsAtK = recallsAtK(re, qrelsDataFrame,n)\n",
    "    print(f'recallsAt@{n} : {recallsAtK}')\n",
    "    MAPAtK =MAPAtK(re, qrelsDataFrame)\n",
    "    print(f'MAPAt : {MAPAtK}')\n",
    "    MRR = MRR(re, qrelsDataFrame)\n",
    "    print(f'MRR : {MRR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390897b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "947c9efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8000\n",
      " * Running on http://10.2.0.2:8000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Jun/2023 20:44:32] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [09/Jun/2023 20:45:19] \"GET /search/1/?query=hi HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from importlib_metadata import method_cache\n",
    "port = 8000\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def initialize():\n",
    "    print(\"initialize\")\n",
    "    try:\n",
    "        init()\n",
    "        return jsonify(success=True)\n",
    "    except:\n",
    "        return jsonify(success=False)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/search/<int:typeSearch>/<string:query>', methods=['GET'])\n",
    "def search(typeSearch,query):\n",
    "    print('search')\n",
    "    try:\n",
    "        res=searchInput(typeSearch,query)\n",
    "        return res  \n",
    "    except:\n",
    "        raise #TODO handle\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d3393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3571fa39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
